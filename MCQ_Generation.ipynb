{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f53ca6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "from typing import List, Optional, Dict, Union\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pydantic import BaseModel, Field, conlist, validator\n",
    "from typing import Dict, List\n",
    "from enum import Enum\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain.schema import Document as SchemaDocument  \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebccbba8",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fd4c183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .env file and set the API key\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Check if key is loaded\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"‚ùå GOOGLE_API_KEY not found in .env\")\n",
    "\n",
    "# Initialize Gemini model for generation\n",
    "mcq_generator = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-preview-05-20\",   # gemini-2.5-flash-preview-05-20\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "# Initialize Gemini model for evaluation\n",
    "mcq_evaluator = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-preview-04-17-thinking\",  # gemini-2.5-flash-preview-04-17-thinking\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1cfbb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello! As an AI, I don\\'t have feelings or a physical body, so I don\\'t get \"how are you\" in the human sense. But I\\'m ready and functioning perfectly!\\n\\nHow are *you* doing today? And what can I help you with?' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run--95b302e8-eeeb-4d94-98e8-db53906cfcf4-0' usage_metadata={'input_tokens': 7, 'output_tokens': 59, 'total_tokens': 102, 'input_token_details': {'cache_read': 0}}\n",
      "content='Hello!\\n\\nAs an AI, I don\\'t have feelings or a physical state like a human does, so I don\\'t experience being \"well\" or \"unwell.\"\\n\\nHowever, I\\'m here and ready to assist you! How can I help you today?' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run--08f2cce5-654c-4e51-88b1-54232fc8b253-0' usage_metadata={'input_tokens': 7, 'output_tokens': 57, 'total_tokens': 506, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(mcq_generator.invoke(\"Hello, how are you?\"))\n",
    "print(mcq_evaluator.invoke(\"Hello, how are you?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17779197",
   "metadata": {},
   "source": [
    "# final function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c564f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for MCQ generation \n",
    "mcq_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    template=\"\"\"\n",
    "You have been given some content. Your task is to generate multiple choice questions (MCQs) that assess deep understanding of the core concepts, reasoning, and applications within that content.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "- Use only the information presented in the given content.\n",
    "- Create questions that require comprehension, interpretation, or application of the main ideas, principles, or reasoning shown in the content.\n",
    "- Focus your questions on the underlying concepts, methods, or logic discussed, rather than on surface details or specific examples.\n",
    "- When examples are present, use them as inspiration to create new, similar scenarios or to ask about the general principle they illustrate, rather than about the specific details or outputs of those examples.\n",
    "- Encourage the learner to think about why or how a concept works, or how it can be applied, rather than recalling isolated facts, names, or figures.\n",
    "- Ensure that all key ideas are covered. Each question should test meaningful understanding or the ability to apply what was conveyed.\n",
    "- Make each question unique, covering a distinct aspect of the content.\n",
    "- Generate as many MCQs as possible from all the key topics in the content.\n",
    "\n",
    "Avoid:\n",
    "\n",
    "- Do NOT ask questions that test memory of specific figures, dates, percentages, or logistical details.\n",
    "- Avoid questions about:\n",
    "  - Administrative details, deadlines, or metadata\n",
    "  - Author names, publication info, or references\n",
    "  - Off-topic content not central to the main ideas\n",
    "\n",
    "For each MCQ:\n",
    "  1. Write a clear and concise question.\n",
    "  2. Provide 4 plausible options labeled A, B, C, and D.\n",
    "  3. Make the incorrect options plausible but clearly wrong.\n",
    "  4. Ensure only one option is correct.\n",
    "  5. Assign a difficulty level: \"easy\", \"medium\", or \"hard\".\n",
    "  6. Add a topic that best categorizes the question.\n",
    "  7. Provide an explanation for each option (why it is correct or incorrect).\n",
    "\n",
    "Maintain this difficulty distribution:\n",
    "  - 20-30% Hard\n",
    "  - 30-40% Medium\n",
    "  - 30-50% Easy\n",
    "\n",
    "Return your answers as a list of valid JSON objects in the following format:\n",
    "  {{\n",
    "    \"status\": \"success\",\n",
    "    \"mcqs\": [\n",
    "              {{\n",
    "              \"question\": \"Sample question?\",\n",
    "              \"options\": {{\n",
    "              \"A\": \"Option A text\",   \n",
    "              \"B\": \"Option B text\",\n",
    "              \"C\": \"Option C text\",\n",
    "              \"D\": \"Option D text\"\n",
    "              }},\n",
    "              \"answer\": \"C\",\n",
    "              \"topic\": \"Relevant topic here\",\n",
    "              \"difficulty\": \"easy\",\n",
    "              \"explanation\": {{\n",
    "              \"A\": \"Option A is incorrect because...\",\n",
    "              \"B\": \"Option B is incorrect because...\",\n",
    "              \"C\": \"Option C is correct because...\",\n",
    "              \"D\": \"Option D is incorrect because...\"\n",
    "              }}\n",
    "              }}\n",
    "            ]\n",
    "  }}\n",
    "  \n",
    "- If content has no generatable MCQs then return :\n",
    "  {{\n",
    "    \"status\": \"no_valid_mcqs\",\n",
    "    \"mcqs\": []\n",
    "  }}     \n",
    "\n",
    "Content:\n",
    "{content}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "mcq_evaluation_prompt = PromptTemplate(\n",
    "    input_variables=[\"chunk\", \"mcqs\"],\n",
    "    template=\"\"\"\n",
    "You are an expert educational content validator. Your role is to rigorously assess and filter a given list of **multiple-choice questions (MCQs)** against a provided **source content chunk**.\n",
    "\n",
    "Your ONLY task is to return a JSON array containing **ONLY high-quality MCQs** that strictly adhere to ALL the evaluation criteria. If an MCQ violates even a single rule, it MUST be excluded. You are NOT allowed to modify, correct, or infer anything about the MCQs.\n",
    "\n",
    "---\n",
    "\n",
    "### FUNDAMENTAL PRINCIPLE FOR ACCEPTANCE:\n",
    "**Every accepted MCQ must test the learner's understanding, application, or reasoning of the CORE ACADEMIC CONCEPTS, principles, theories, or methods explicitly discussed in the `chunk`. Questions about the *delivery, administration, or structure* of the course itself are strictly forbidden.**\n",
    "\n",
    "---\n",
    "\n",
    "### EVALUATION CRITERIA (All Must Be Satisfied for an MCQ to be Accepted):\n",
    "\n",
    "1.  **Direct Content Basis:** The question and its correct answer MUST be directly derivable and supported by the explicit information within the provided `chunk` of content. No outside knowledge or interpretation allowed.\n",
    "\n",
    "2.  **Plausible Distractors:** All incorrect options (distractors) must be genuinely plausible and relevant to the subject matter, but unequivocally wrong based on the `chunk`. They should not be obviously incorrect or irrelevant.\n",
    "\n",
    "3.  **Unambiguous Correct Answer:** There must be *exactly one* correct answer, and it must be unequivocally correct based on the `chunk`.\n",
    "\n",
    "4.  **Accurate & Specific Topic:** The `topic` field must accurately and precisely reflect the core academic subject or concept the question is testing, as presented in the `chunk`. It should *never* refer to course logistics or structure.\n",
    "\n",
    "5.  **Comprehensive Explanations:** Every option (A, B, C, D) must have a concise and accurate `explanation` detailing *why* it is correct or incorrect, specifically referencing the content in the `chunk`.\n",
    "\n",
    "6.  **No Redundancy/Duplicates:** The MCQ must be unique. It should not repeat concepts, question phrasing, or the underlying reasoning tested by any other MCQ in the list. Avoid questions that are mere rephrasing of another.\n",
    "\n",
    "7.  **Depth of Understanding (Conceptual Focus):** The question must assess understanding, reasoning, analysis, or application of core academic concepts. **STRICTLY REJECT** questions that primarily test:\n",
    "    * Memorization of isolated facts (e.g., exact numbers, dates, specific names, definitions without context, simple recall of lists).\n",
    "    * Logistical details (e.g., deadlines, submission methods, formatting rules).\n",
    "    * Administrative details (e.g., class policies, grading, attendance).\n",
    "\n",
    "---\n",
    "\n",
    "### CRITICAL RED FLAGS: IMMEDIATELY REJECT MCQs IF THEY PERTAIN TO ANY OF THE FOLLOWING:\n",
    "\n",
    "* **Course Logistics/Administration:** Questions about how the course is run, organized, graded, or what students are \"required\" to do beyond learning the subject matter.\n",
    "    * *Examples to reject:* \"What is the policy for late submissions?\", \"How many assignments are there?\", \"What is the recommended study strategy for this course?\", \"What is the purpose of the final project *proposal*?\", \"Why are students required to scribe lectures?\", \"What is the pedagogical reason for X course policy?\"\n",
    "\n",
    "* **Pedagogical Intent/Teaching Methods:** Questions about *why* the instructor chose a certain teaching method, assignment type, or course structure.\n",
    "    * *Examples to reject:* \"What is the aim of using case studies in this module?\", \"What is the instructor's philosophy behind peer review?\", \"Why is there a page limit for P-sets?\"\n",
    "\n",
    "* **Meta-Cognition/Learning Process:** Questions asking about the *process of learning* the content within the course context.\n",
    "    * *Examples to reject:* \"What is the best way to prepare for the exam?\", \"What skills will this course help you develop?\"\n",
    "\n",
    "* **Subjective Opinions or Preferences:** Questions that can have multiple subjective answers or ask for opinions.\n",
    "\n",
    "* **Referencing External Course Elements:** Questions that refer to \"this video,\" \"the lecture notes,\" \"Module 3,\" \"the textbook,\" etc., rather than directly extracting knowledge from the provided `chunk`.\n",
    "\n",
    "---\n",
    "\n",
    "### CONTENT CHUNK:\n",
    "{chunk}\n",
    "\n",
    "---\n",
    "\n",
    "### GENERATED MCQs:\n",
    "{mcqs}\n",
    "\n",
    "---\n",
    "\n",
    "### FINAL OUTPUT INSTRUCTIONS:\n",
    "\n",
    "-   If one or more valid MCQs remain after filtering, return a **clean JSON array** of valid MCQs only. Do NOT include any markdown, commentary, or extra text.\n",
    "  {{\n",
    "    \"status\": \"success\",\n",
    "    \"mcqs\": [\n",
    "              {{\n",
    "              \"question\": \"Sample question?\",\n",
    "              \"options\": {{\n",
    "              \"A\": \"Option A text\",   \n",
    "              \"B\": \"Option B text\",\n",
    "              \"C\": \"Option C text\",\n",
    "              \"D\": \"Option D text\"\n",
    "              }},\n",
    "              \"answer\": \"C\",\n",
    "              \"topic\": \"Relevant topic here\",\n",
    "              \"difficulty\": \"easy\",\n",
    "              \"explanation\": {{\n",
    "              \"A\": \"Option A is incorrect because...\",\n",
    "              \"B\": \"Option B is incorrect because...\",\n",
    "              \"C\": \"Option C is correct because...\",\n",
    "              \"D\": \"Option D is incorrect because...\"\n",
    "              }}\n",
    "              }}\n",
    "            ]\n",
    "  }}\n",
    "  \n",
    "- If content has no generatable MCQs then return :\n",
    "  {{\n",
    "    \"status\": \"no_valid_mcqs\",\n",
    "    \"mcqs\": []\n",
    "  }}     \n",
    "\n",
    "\n",
    "**IMPORTANT:** Your output must be ONLY valid JSON. No pre-text, no post-text, no explanations, no markdown fences unless specifically requested for the JSON itself.\n",
    "\"\"\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c2f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_valid_mcq_structure(data):\n",
    "    \"\"\"\n",
    "    Checks if the parsed data has a 'status' key and a 'mcqs' list with proper MCQ dict structure.\n",
    "    \"\"\"\n",
    "    required_keys = {\"question\", \"options\", \"answer\", \"topic\", \"difficulty\", \"explanation\"}\n",
    "\n",
    "    if not isinstance(data, dict):\n",
    "        print(\"Top-level data is not a dict\")\n",
    "        return False\n",
    "\n",
    "    if data.get(\"status\") != \"success\":\n",
    "        print(\"Status is not 'success'\")\n",
    "        return False\n",
    "\n",
    "    mcqs = data.get(\"mcqs\")\n",
    "    if not isinstance(mcqs, list):\n",
    "        print(\"'mcqs' field is not a list\")\n",
    "        return False\n",
    "\n",
    "    for item in mcqs:\n",
    "        if not isinstance(item, dict):\n",
    "            print(\"An MCQ is not a dict\")\n",
    "            return False\n",
    "        if not required_keys.issubset(item.keys()):\n",
    "            print(\"Required keys missing in MCQ\")\n",
    "            return False\n",
    "        # Keeping original validation for options and explanation as dicts\n",
    "        if not isinstance(item[\"options\"], dict) or not isinstance(item[\"explanation\"], dict):\n",
    "            print(\"'options' or 'explanation' is not a dict\")\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def is_reviewer_rejection(data):\n",
    "    if isinstance(data, dict):  print(data.get(\"status\"))\n",
    "    return isinstance(data, dict) and data.get(\"status\") == \"no_valid_mcqs\"\n",
    "\n",
    "\n",
    "def safe_json_extract(output):\n",
    "    print(\"parser is running\")\n",
    "    \n",
    "    if isinstance(output, (list, dict)):\n",
    "        if is_valid_mcq_structure(output):\n",
    "            return output\n",
    "        elif is_reviewer_rejection(output):\n",
    "            return output\n",
    "        else:\n",
    "            print(\"Returning error in status.\")\n",
    "            print(\"output was :\",output)\n",
    "            return {\"status\": \"error\", \"mcqs\": []}   # still allows retry\n",
    "\n",
    "    try:\n",
    "        if hasattr(output, 'content'):\n",
    "            output = output.content\n",
    "\n",
    "        # Find the first opening curly brace '{' and slice the string from there\n",
    "        first_brace_index = output.find('{')\n",
    "        if first_brace_index != -1:\n",
    "            output = output[first_brace_index:]\n",
    "        else:\n",
    "            # If no opening brace is found, it's not valid JSON.\n",
    "            # Log an error and return the error structure.\n",
    "            print(\"‚ö†Ô∏è No opening '{' found in the output string. Cannot parse as JSON.\")\n",
    "            print(\"output was :\",output)\n",
    "            return {\"status\": \"error\", \"mcqs\": []}\n",
    "\n",
    "        \n",
    "        # Cleanup steps (these remain as in your original code)\n",
    "        output = re.sub(r\"```(?:json)?\", \"\", output).strip(\"` \\n\")\n",
    "        output = re.sub(r\",\\s*([}\\]])\", r\"\\1\", output)\n",
    "        output = re.sub(r'[\\x00-\\x09\\x0B-\\x1F]', '', output)\n",
    "        output = re.sub(r'(?<={|,)\\s*(\\w+)\\s*:', r'\"\\1\":', output)\n",
    "\n",
    "        parsed = json.loads(output)\n",
    "        \n",
    "        if is_valid_mcq_structure(parsed):\n",
    "            # print(4)\n",
    "            return parsed[\"mcqs\"]\n",
    "        elif is_reviewer_rejection(parsed):\n",
    "            # print(5)\n",
    "            return parsed\n",
    "        else:\n",
    "            print(\"Returning statues as error and output was as bellow:\")\n",
    "            print(output)\n",
    "            return {\"status\": \"error\", \"mcqs\": []}   # will trigger retry\n",
    "\n",
    "    except Exception as e:\n",
    "        with open(\"malformed_output.json\", \"w\") as f:\n",
    "            f.write(str(output)) # Ensure output is a string when writing\n",
    "        print(f\"‚ö†Ô∏è JSON parsing failed: {e}\")\n",
    "        return {\"status\": \"error\", \"mcqs\": []}\n",
    "\n",
    "\n",
    "\n",
    "# Wrap it as a LangChain Runnable\n",
    "json_parser = RunnableLambda(safe_json_extract)\n",
    "\n",
    "\n",
    "def load_and_split_file(file_path: Optional[str] = None,\n",
    "                        transcript: Optional[str] = None,\n",
    "                        chunk_size: int = 4000,\n",
    "                        chunk_overlap: int = 200 ) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load a .txt/.pdf file or a raw transcript string and split it into chunks using LangChain's RecursiveCharacterTextSplitter.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str, optional): Path to the input file (.txt or .pdf). Either this or `transcript` must be provided.\n",
    "        transcript (str, optional): Raw transcript string. Used if no file_path is given.\n",
    "        chunk_size (int): Maximum size (in characters) of each text chunk.\n",
    "        chunk_overlap (int): Number of characters to overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of LangChain Document objects split from the input.\n",
    "    \"\"\"\n",
    "    if transcript:\n",
    "        combined_text = transcript\n",
    "        source = \"transcript_input\"\n",
    "    elif file_path:\n",
    "        ext = os.path.splitext(file_path)[-1].lower()\n",
    "\n",
    "        if ext == \".txt\":\n",
    "            loader = TextLoader(file_path)\n",
    "        elif ext == \".pdf\":\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type. Only .txt and .pdf are supported.\")\n",
    "\n",
    "        documents = loader.load()\n",
    "\n",
    "        # Combine all page contents into one string\n",
    "        combined_text = \"\\n\".join(doc.page_content for doc in documents)\n",
    "        print(\"total character :\",len(combined_text))\n",
    "        source = file_path\n",
    "    else:\n",
    "        raise ValueError(\"Either file_path or transcript must be provided.\")\n",
    "\n",
    "    # Create a single Document with the combined content\n",
    "    full_document = [Document(page_content=combined_text, metadata={\"source\": source})]\n",
    "    \n",
    "    # Now chunk this single full document\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = splitter.split_documents(full_document)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def generate_mcqs_from_file_with_eval(file_path=None, transcript=None, chunk_size=4000, chunk_overlap = 200, max_retries=3):\n",
    "    \"\"\"\n",
    "    Generate and evaluate MCQs from either a file or a raw transcript using two LLM pipelines.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str, optional): Path to input .txt or .pdf file.\n",
    "        transcript (str, optional): Raw string containing the transcript text.\n",
    "        chunk_size (int): Maximum chunk size to split the document.\n",
    "        max_retries (int): Maximum number of retries for a failed chunk.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: A list of final validated MCQs extracted and reviewed from the input.\n",
    "    \"\"\"\n",
    "\n",
    "    chunks = load_and_split_file(file_path=file_path, transcript=transcript, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    generation_chain = mcq_prompt_template | mcq_generator | json_parser\n",
    "    review_chain = mcq_evaluation_prompt | mcq_evaluator | json_parser\n",
    "\n",
    "    all_mcqs = []\n",
    "    all_raw_mcqs = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # if i == 1:\n",
    "        #     break\n",
    "        print(f\"\\nüîπ Processing chunk {i + 1}/{len(chunks)}...\")\n",
    "        content = chunk.page_content.strip()\n",
    "        attempt = 0\n",
    "\n",
    "        while attempt < max_retries:\n",
    "            try:\n",
    "                # Step 1: Generate raw MCQs\n",
    "                print(\"Attempt : \",attempt + 1)\n",
    "                raw_mcqs = generation_chain.invoke({\"content\": content})\n",
    "                if not raw_mcqs:\n",
    "                    raise ValueError(\"‚ö†Ô∏è No MCQs generated\")\n",
    "\n",
    "                if isinstance(raw_mcqs, dict) and raw_mcqs.get(\"status\") == \"error\":\n",
    "                    print(\"error in parser\")\n",
    "                    attempt += 1\n",
    "                    continue\n",
    "\n",
    "                if is_reviewer_rejection(raw_mcqs):\n",
    "                    print(f\"üü° Chunk {i+1}: chunk has no relevant MCQs (not retrying).\")\n",
    "                    break\n",
    "               \n",
    "                print(\"raw mcqs : \",len(raw_mcqs))\n",
    "\n",
    "                all_raw_mcqs.extend(raw_mcqs)\n",
    "                \n",
    "                if isinstance(raw_mcqs, str):\n",
    "                    raw_mcqs = json.loads(raw_mcqs)\n",
    "\n",
    "                # Step 2: Evaluate and filter MCQs\n",
    "                final_mcqs = review_chain.invoke({\n",
    "                    \"chunk\": content,\n",
    "                    \"mcqs\": raw_mcqs\n",
    "                })\n",
    "                \n",
    "                if isinstance(final_mcqs, dict) and final_mcqs.get(\"status\") == \"error\":\n",
    "                    print(\"error in parser\")\n",
    "                    attempt += 1\n",
    "                    continue\n",
    "                \n",
    "                if is_reviewer_rejection(final_mcqs):\n",
    "                    print(f\"üü° Chunk {i+1}: Reviewer rejected all MCQs (not retrying).\")\n",
    "                    break\n",
    "                elif not final_mcqs:\n",
    "                    print(final_mcqs)\n",
    "                    raise ValueError(\"‚ùå Review returned no valid MCQs\")\n",
    "\n",
    "                all_mcqs.extend(final_mcqs)\n",
    "                print(f\"‚úÖ Chunk {i + 1}: {len(final_mcqs)} valid MCQs. Total so far: {len(all_mcqs)} and raw MCQs are {len(all_raw_mcqs)}.\")\n",
    "                break  # ‚úÖ Exit retry loop\n",
    "\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                print(f\"[Retry {attempt}/{max_retries}] Error in chunk {i + 1}: {str(e)}\")\n",
    "                if attempt < max_retries:\n",
    "                    wait_time = 2 ** attempt  # Exponential backoff\n",
    "                    print(f\"‚è≥ Retrying after {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"üö´ Skipping chunk {i + 1} after {max_retries} failed attempts.\")\n",
    "                    break\n",
    "\n",
    "        time.sleep(3)  \n",
    "    return all_mcqs,all_raw_mcqs\n",
    "\n",
    "def deeply_sorted(obj):\n",
    "    \"\"\"Recursively sort any dictionary to ensure consistent comparison.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: deeply_sorted(obj[k]) for k in sorted(obj)}\n",
    "    elif isinstance(obj, list):\n",
    "        return [deeply_sorted(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def get_rejected_mcqs_from_lists(initial_mcqs, evaluated_mcqs):\n",
    "    \"\"\"\n",
    "    Return unique MCQs from initial_mcqs that are not in evaluated_mcqs,\n",
    "    preserving first appearance order.\n",
    "    \"\"\"\n",
    "    evaluated_serialized = {\n",
    "        json.dumps(deeply_sorted(mcq), separators=(\",\", \":\"))\n",
    "        for mcq in evaluated_mcqs\n",
    "    }\n",
    "\n",
    "    seen = set()\n",
    "    rejected_mcqs = []\n",
    "    for mcq in initial_mcqs:\n",
    "        mcq_str = json.dumps(deeply_sorted(mcq), separators=(\",\", \":\"))\n",
    "        if mcq_str not in evaluated_serialized and mcq_str not in seen:\n",
    "            rejected_mcqs.append(mcq)\n",
    "            seen.add(mcq_str)\n",
    "\n",
    "    return rejected_mcqs\n",
    "\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"\n",
    "    Saves a list or dictionary to a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "        data (list or dict): The data to be saved (e.g., list of MCQs).\n",
    "        filename (str): The name of the output JSON file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Write data to file in pretty-printed JSON format\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error saving JSON:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c955723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total character : 5800\n",
      "\n",
      "üîπ Processing chunk 1/2...\n",
      "Attempt :  1\n",
      "parser is running\n",
      "raw mcqs :  6\n",
      "parser is running\n",
      "‚úÖ Chunk 1: 6 valid MCQs. Total so far: 6 and raw MCQs are 6.\n",
      "\n",
      "üîπ Processing chunk 2/2...\n",
      "Attempt :  1\n",
      "parser is running\n",
      "raw mcqs :  5\n",
      "parser is running\n",
      "‚úÖ Chunk 2: 5 valid MCQs. Total so far: 11 and raw MCQs are 11.\n",
      "Saved to Multiply_Matrices_rejected_1_p2.json\n",
      "Saved to Multiply_Matrices_1_p2.json\n",
      "Saved to Multiply_Matrices_raw_1_p2.json\n"
     ]
    }
   ],
   "source": [
    "path = r\"experiment\\Matrix muliplication\\Multiply_Matrices.txt\"\n",
    "mcqs,raw = generate_mcqs_from_file_with_eval(file_path = path,chunk_size=4000)\n",
    "rejected = get_rejected_mcqs_from_lists(raw,mcqs)\n",
    "save_to_json(rejected, \"Multiply_Matrices_rejected_1_p2.json\")\n",
    "save_to_json(mcqs, \"Multiply_Matrices_1_p2.json\")\n",
    "save_to_json(raw, \"Multiply_Matrices_raw_1_p2.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a090cfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total character : 44738\n",
      "\n",
      "üîπ Processing chunk 1/12...\n",
      "Attempt :  1\n",
      "parser is running\n",
      "raw mcqs :  7\n",
      "parser is running\n",
      "Status is not 'success'\n",
      "no_valid_mcqs\n",
      "no_valid_mcqs\n",
      "üü° Chunk 1: Reviewer rejected all MCQs (not retrying).\n",
      "\n",
      "üîπ Processing chunk 2/12...\n",
      "Attempt :  1\n",
      "parser is running\n",
      "raw mcqs :  7\n",
      "parser is running\n",
      "‚úÖ Chunk 2: 6 valid MCQs. Total so far: 6 and raw MCQs are 14.\n",
      "\n",
      "üîπ Processing chunk 3/12...\n",
      "Attempt :  1\n",
      "parser is running\n",
      "raw mcqs :  11\n",
      "parser is running\n",
      "‚úÖ Chunk 3: 11 valid MCQs. Total so far: 17 and raw MCQs are 25.\n",
      "\n",
      "üîπ Processing chunk 4/12...\n",
      "Attempt :  1\n",
      "parser is running\n",
      "raw mcqs :  8\n",
      "parser is running\n",
      "‚úÖ Chunk 4: 8 valid MCQs. Total so far: 25 and raw MCQs are 33.\n",
      "\n",
      "üîπ Processing chunk 5/12...\n",
      "Attempt :  1\n",
      "parser is running\n",
      "raw mcqs :  10\n",
      "parser is running\n",
      "‚úÖ Chunk 5: 9 valid MCQs. Total so far: 34 and raw MCQs are 43.\n",
      "\n",
      "üîπ Processing chunk 6/12...\n",
      "Attempt :  1\n",
      "parser is running\n",
      "raw mcqs :  13\n",
      "parser is running\n",
      "‚úÖ Chunk 6: 12 valid MCQs. Total so far: 46 and raw MCQs are 56.\n",
      "\n",
      "üîπ Processing chunk 7/12...\n",
      "Attempt :  1\n",
      "parser is running\n",
      "raw mcqs :  9\n",
      "parser is running\n",
      "‚úÖ Chunk 7: 8 valid MCQs. Total so far: 54 and raw MCQs are 65.\n",
      "\n",
      "üîπ Processing chunk 8/12...\n",
      "Attempt :  1\n",
      "parser is running\n",
      "raw mcqs :  8\n",
      "parser is running\n",
      "‚úÖ Chunk 8: 8 valid MCQs. Total so far: 62 and raw MCQs are 73.\n",
      "\n",
      "üîπ Processing chunk 9/12...\n",
      "Attempt :  1\n",
      "parser is running\n",
      "raw mcqs :  11\n",
      "parser is running\n",
      "‚úÖ Chunk 9: 11 valid MCQs. Total so far: 73 and raw MCQs are 84.\n",
      "\n",
      "üîπ Processing chunk 10/12...\n",
      "Attempt :  1\n",
      "parser is running\n",
      "raw mcqs :  10\n",
      "parser is running\n",
      "‚úÖ Chunk 10: 10 valid MCQs. Total so far: 83 and raw MCQs are 94.\n",
      "\n",
      "üîπ Processing chunk 11/12...\n",
      "Attempt :  1\n",
      "parser is running\n",
      "raw mcqs :  9\n",
      "parser is running\n",
      "‚úÖ Chunk 11: 7 valid MCQs. Total so far: 90 and raw MCQs are 103.\n",
      "\n",
      "üîπ Processing chunk 12/12...\n",
      "Attempt :  1\n",
      "parser is running\n",
      "raw mcqs :  10\n",
      "parser is running\n",
      "‚úÖ Chunk 12: 10 valid MCQs. Total so far: 100 and raw MCQs are 113.\n",
      "Saved to Advanced_Algorithms_1_p2.json\n",
      "Saved to Advanced_Algorithms_raw_1_p2.json\n",
      "Saved to Advanced_Algorithms_rejected_1_p2.json\n"
     ]
    }
   ],
   "source": [
    "path = r\"experiment\\Advanced Algorithms\\Advanced_Algorithms.txt\"\n",
    "mcqs,raw = generate_mcqs_from_file_with_eval(file_path = path,chunk_size=4000)\n",
    "rejected = get_rejected_mcqs_from_lists(raw,mcqs)\n",
    "save_to_json(mcqs, \"Advanced_Algorithms_1_p2.json\")\n",
    "save_to_json(raw, \"Advanced_Algorithms_raw_1_p2.json\")\n",
    "save_to_json(rejected, \"Advanced_Algorithms_rejected_1_p2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54793c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total character : 9752\n",
      "\n",
      "üîπ Processing chunk 1/3...\n",
      "parser is running\n",
      "raw mcqs :  10\n",
      "parser is running\n",
      "‚úÖ Chunk 1: 10 valid MCQs. Total so far: 10 and raw MCQs are 10.\n",
      "Saved to IP_protection_policy_1_p2.json\n"
     ]
    }
   ],
   "source": [
    "mcqs,raw = generate_mcqs_from_file_with_eval(file_path = path,chunk_size=4000)\n",
    "save_to_json(mcqs, \"IP_protection_policy_1_p2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba25e795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total character : 9752\n",
      "\n",
      "üîπ Processing chunk 1/3...\n",
      "parser is running\n",
      "raw mcqs :  12\n",
      "parser is running\n",
      "‚úÖ Chunk 1: 12 valid MCQs. Total so far: 12 and raw MCQs are 12.\n",
      "\n",
      "üîπ Processing chunk 2/3...\n",
      "parser is running\n",
      "raw mcqs :  12\n",
      "parser is running\n",
      "‚úÖ Chunk 2: 12 valid MCQs. Total so far: 24 and raw MCQs are 24.\n",
      "\n",
      "üîπ Processing chunk 3/3...\n",
      "parser is running\n",
      "raw mcqs :  12\n",
      "parser is running\n",
      "‚úÖ Chunk 3: 12 valid MCQs. Total so far: 36 and raw MCQs are 36.\n",
      "Saved to IP_protection_policy_rejected_1_p2.json\n",
      "Saved to IP_protection_policy_1_p2.json\n",
      "Saved to IP_protection_policy_raw_1_p2.json\n"
     ]
    }
   ],
   "source": [
    "path = r\"experiment\\IP_Protection_Policy\\IP_Protection_Policy.pdf\"\n",
    "mcqs,raw = generate_mcqs_from_file_with_eval(file_path = path,chunk_size=4000)\n",
    "rejected = get_rejected_mcqs_from_lists(raw,mcqs)\n",
    "save_to_json(rejected, \"IP_protection_policy_rejected_1_p2.json\")\n",
    "save_to_json(mcqs, \"IP_protection_policy_1_p2.json\")\n",
    "save_to_json(raw, \"IP_protection_policy_raw_1_p2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8153fafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total character : 13303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r\"C:\\Users\\admin\\OneDrive\\Desktop\\coriolis\\VS CODE\\experiment\\employee-leave-policy (1).pdf\"\n",
    "chunks = load_and_split_file(file_path = path )\n",
    "len(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec60326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total character : 13303\n",
      "\n",
      "üîπ Processing chunk 1/6...\n",
      "parser is running\n",
      "üü° Chunk 1: chunk has no relevant MCQs (not retrying).\n",
      "\n",
      "üîπ Processing chunk 2/6...\n",
      "parser is running\n",
      " Raw response : The provided content is a table of contents, listing only headings and sub-headings for different types of leave. It does not contain any substantive information, definitions, principles, processes, or values for eligibility, entitlement, or any other aspect of the leave types.\n",
      "\n",
      "To generate MCQs that assess \"deep understanding of the core concepts, reasoning, and applications,\" as per the instructions, there must be actual content describing these concepts, principles, or their application. Since the content is purely structural and devoid of any explanatory text, it is not possible to create questions that go beyond simple recall of the document's structure (e.g., \"Which section is listed under Earned Leave?\"), which would violate the instruction to \"avoid questions that test memory of specific figures, dates, percentages, or logistical details\" and \"rather than recalling isolated facts.\"\n",
      "\n",
      "Therefore, no valid MCQs can be generated based on the given content that meet the specified criteria for assessing deep understanding, reasoning, or application.\n",
      "\n",
      "\n",
      "{\n",
      "  \"status\": \"no_valid_mcqs\"\n",
      "}...\n",
      "‚ö†Ô∏è JSON parsing failed: Expecting value: line 1 column 1 (char 0)\n",
      "[Retry 1/3] Error in chunk 2: ‚ö†Ô∏è No MCQs generated\n",
      "‚è≥ Retrying after 2 seconds...\n",
      "parser is running\n",
      "üü° Chunk 2: chunk has no relevant MCQs (not retrying).\n",
      "\n",
      "üîπ Processing chunk 3/6...\n",
      "parser is running\n",
      "üü° Chunk 3: chunk has no relevant MCQs (not retrying).\n",
      "\n",
      "üîπ Processing chunk 4/6...\n",
      "parser is running\n",
      "raw mcqs :  13\n",
      "parser is running\n",
      "‚úÖ Chunk 4: 13 valid MCQs. Total so far: 13 and raw MCQs are 13.\n",
      "\n",
      "üîπ Processing chunk 5/6...\n",
      "parser is running\n",
      "raw mcqs :  17\n",
      "parser is running\n",
      "‚úÖ Chunk 5: 17 valid MCQs. Total so far: 30 and raw MCQs are 30.\n",
      "\n",
      "üîπ Processing chunk 6/6...\n",
      "parser is running\n",
      "raw mcqs :  7\n",
      "parser is running\n",
      "‚úÖ Chunk 6: 6 valid MCQs. Total so far: 36 and raw MCQs are 37.\n",
      "Saved to leave_policy_3_rejected_1_p2.json\n",
      "Saved to leave_policy_3_1_p2.json\n",
      "Saved to leave_policy_3_raw_1_p2.json\n"
     ]
    }
   ],
   "source": [
    "path = r\"experiment\\employee-leave-policy\\employee-leave-policy.pdf\"\n",
    "mcqs,raw = generate_mcqs_from_file_with_eval(file_path = path,chunk_size=4000)\n",
    "rejected = get_rejected_mcqs_from_lists(raw,mcqs)\n",
    "save_to_json(rejected, \"leave_policy_3_rejected_1_p2.json\")\n",
    "save_to_json(mcqs, \"leave_policy_3_1_p2.json\")\n",
    "save_to_json(raw, \"leave_policy_3_raw_1_p2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68206a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rejected = get_rejected_mcqs_from_lists(raw,mcqs)\n",
    "len(rejected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0285d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total character : 39474\n",
      "\n",
      "üîπ Processing chunk 1/11...\n",
      "parser is running\n",
      "raw mcqs :  11\n",
      "parser is running\n",
      "‚úÖ Chunk 1: 11 valid MCQs. Total so far: 11 and raw MCQs are 11.\n",
      "\n",
      "üîπ Processing chunk 2/11...\n",
      "parser is running\n",
      "raw mcqs :  7\n",
      "parser is running\n",
      "‚úÖ Chunk 2: 7 valid MCQs. Total so far: 18 and raw MCQs are 18.\n",
      "\n",
      "üîπ Processing chunk 3/11...\n",
      "parser is running\n",
      "raw mcqs :  10\n",
      "parser is running\n",
      "‚úÖ Chunk 3: 10 valid MCQs. Total so far: 28 and raw MCQs are 28.\n",
      "\n",
      "üîπ Processing chunk 4/11...\n",
      "parser is running\n",
      "raw mcqs :  8\n",
      "parser is running\n",
      "‚úÖ Chunk 4: 8 valid MCQs. Total so far: 36 and raw MCQs are 36.\n",
      "\n",
      "üîπ Processing chunk 5/11...\n",
      "parser is running\n",
      "raw mcqs :  10\n",
      "parser is running\n",
      "‚úÖ Chunk 5: 8 valid MCQs. Total so far: 44 and raw MCQs are 46.\n",
      "\n",
      "üîπ Processing chunk 6/11...\n",
      "parser is running\n",
      "raw mcqs :  6\n",
      "parser is running\n",
      "‚úÖ Chunk 6: 6 valid MCQs. Total so far: 50 and raw MCQs are 52.\n",
      "\n",
      "üîπ Processing chunk 7/11...\n",
      "parser is running\n",
      "raw mcqs :  9\n",
      "parser is running\n",
      "‚úÖ Chunk 7: 8 valid MCQs. Total so far: 58 and raw MCQs are 61.\n",
      "\n",
      "üîπ Processing chunk 8/11...\n",
      "parser is running\n",
      "raw mcqs :  10\n",
      "parser is running\n",
      "‚úÖ Chunk 8: 10 valid MCQs. Total so far: 68 and raw MCQs are 71.\n",
      "\n",
      "üîπ Processing chunk 9/11...\n",
      "parser is running\n",
      "raw mcqs :  9\n",
      "parser is running\n",
      "‚úÖ Chunk 9: 8 valid MCQs. Total so far: 76 and raw MCQs are 80.\n",
      "\n",
      "üîπ Processing chunk 10/11...\n",
      "parser is running\n",
      "raw mcqs :  10\n",
      "parser is running\n",
      "‚úÖ Chunk 10: 10 valid MCQs. Total so far: 86 and raw MCQs are 90.\n",
      "\n",
      "üîπ Processing chunk 11/11...\n",
      "parser is running\n",
      "raw mcqs :  4\n",
      "parser is running\n",
      "‚úÖ Chunk 11: 4 valid MCQs. Total so far: 90 and raw MCQs are 94.\n",
      "Saved to madhvan_heaps_rejected.json\n",
      "Saved to madhvan_heaps.json\n",
      "Saved to madhvan_heaps_raw.json\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:\\Users\\admin\\OneDrive\\Desktop\\coriolis\\VS CODE\\experiment\\madhvan_heaps_big_trnscpt.txt\"\n",
    "mcqs,raw = generate_mcqs_from_file_with_eval(file_path = path,chunk_size=4000)\n",
    "rejected = get_rejected_mcqs_from_lists(raw,mcqs)\n",
    "save_to_json(rejected,\"madhvan_heaps_rejected.json\")\n",
    "save_to_json(mcqs,\"madhvan_heaps.json\")\n",
    "save_to_json(raw,\"madhvan_heaps_raw.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bd41dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total character : 41771\n",
      "\n",
      "üîπ Processing chunk 1/11...\n",
      "parser is running\n",
      "raw mcqs :  7\n",
      "parser is running\n",
      "‚úÖ Chunk 1: 7 valid MCQs. Total so far: 7 and raw MCQs are 7.\n",
      "\n",
      "üîπ Processing chunk 2/11...\n",
      "parser is running\n",
      "raw mcqs :  7\n",
      "parser is running\n",
      "‚úÖ Chunk 2: 7 valid MCQs. Total so far: 14 and raw MCQs are 14.\n",
      "\n",
      "üîπ Processing chunk 3/11...\n",
      "parser is running\n",
      "raw mcqs :  7\n",
      "parser is running\n",
      "‚úÖ Chunk 3: 7 valid MCQs. Total so far: 21 and raw MCQs are 21.\n",
      "\n",
      "üîπ Processing chunk 4/11...\n",
      "parser is running\n",
      "raw mcqs :  8\n",
      "parser is running\n",
      "‚úÖ Chunk 4: 8 valid MCQs. Total so far: 29 and raw MCQs are 29.\n",
      "\n",
      "üîπ Processing chunk 5/11...\n",
      "parser is running\n",
      "raw mcqs :  8\n",
      "parser is running\n",
      "‚úÖ Chunk 5: 8 valid MCQs. Total so far: 37 and raw MCQs are 37.\n",
      "\n",
      "üîπ Processing chunk 6/11...\n",
      "parser is running\n",
      "raw mcqs :  9\n",
      "parser is running\n",
      "‚úÖ Chunk 6: 9 valid MCQs. Total so far: 46 and raw MCQs are 46.\n",
      "\n",
      "üîπ Processing chunk 7/11...\n",
      "parser is running\n",
      "raw mcqs :  7\n",
      "parser is running\n",
      "‚úÖ Chunk 7: 6 valid MCQs. Total so far: 52 and raw MCQs are 53.\n",
      "\n",
      "üîπ Processing chunk 8/11...\n",
      "parser is running\n",
      "raw mcqs :  5\n",
      "parser is running\n",
      "‚úÖ Chunk 8: 5 valid MCQs. Total so far: 57 and raw MCQs are 58.\n",
      "\n",
      "üîπ Processing chunk 9/11...\n",
      "parser is running\n",
      "raw mcqs :  12\n",
      "parser is running\n",
      "‚úÖ Chunk 9: 10 valid MCQs. Total so far: 67 and raw MCQs are 70.\n",
      "\n",
      "üîπ Processing chunk 10/11...\n",
      "parser is running\n",
      "raw mcqs :  9\n",
      "parser is running\n",
      "‚úÖ Chunk 10: 9 valid MCQs. Total so far: 76 and raw MCQs are 79.\n",
      "\n",
      "üîπ Processing chunk 11/11...\n",
      "parser is running\n",
      "raw mcqs :  7\n",
      "parser is running\n",
      "‚úÖ Chunk 11: 7 valid MCQs. Total so far: 83 and raw MCQs are 86.\n",
      "Saved to Birthday_Statistics_110_rejected.json\n",
      "Saved to Birthday_Statistics_110.json\n",
      "Saved to Birthday_Statistics_110_raw.json\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:\\Users\\admin\\OneDrive\\Desktop\\coriolis\\VS CODE\\experiment\\Lecture 3_ Birthday Problem, Properties of Probability _ Statistics 110.txt\"\n",
    "mcqs,raw = generate_mcqs_from_file_with_eval(file_path = path,chunk_size=4000)\n",
    "rejected = get_rejected_mcqs_from_lists(raw,mcqs)\n",
    "save_to_json(rejected, \"Birthday_Statistics_110_rejected.json\" )\n",
    "save_to_json(mcqs, \"Birthday_Statistics_110.json\" )\n",
    "save_to_json(raw, \"Birthday_Statistics_110_raw.json\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef6d868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total character : 11297\n",
      "\n",
      "üîπ Processing chunk 1/3...\n",
      "parser is running\n",
      "raw mcqs :  11\n",
      "parser is running\n",
      "‚úÖ Chunk 1: 11 valid MCQs. Total so far: 11 and raw MCQs are 11.\n",
      "\n",
      "üîπ Processing chunk 2/3...\n",
      "parser is running\n",
      "raw mcqs :  6\n",
      "parser is running\n",
      "‚úÖ Chunk 2: 6 valid MCQs. Total so far: 17 and raw MCQs are 17.\n",
      "\n",
      "üîπ Processing chunk 3/3...\n",
      "parser is running\n",
      "raw mcqs :  9\n",
      "parser is running\n",
      "‚úÖ Chunk 3: 9 valid MCQs. Total so far: 26 and raw MCQs are 26.\n",
      "Saved to loop_lecture_3_rejected.json\n",
      "Saved to loop_lecture_3.json\n",
      "Saved to loop_lecture_3_raw.json\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:\\Users\\admin\\OneDrive\\Desktop\\coriolis\\VS CODE\\experiment\\loop_lecture_3.txt\"\n",
    "mcqs,raw = generate_mcqs_from_file_with_eval(file_path = path,chunk_size=4000)\n",
    "rejected = get_rejected_mcqs_from_lists(raw,mcqs)\n",
    "save_to_json(rejected, \"loop_lecture_3_rejected.json\" )\n",
    "save_to_json(mcqs, \"loop_lecture_3.json\" )\n",
    "save_to_json(raw, \"loop_lecture_3_raw.json\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217e14e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total character : 5649\n",
      "Hello, people from the future welcome to Normalized Nerd! Today we‚Äôll set up our camp in the Random Forest. First, we‚Äôll see why the random forest is better than our good old decision trees, and then I‚Äôll explain how it works with visualizations. If you wanna see more videos like this, please subscribe to my channel and hit the bell icon because I make videos about machine learning and data science regularly. So without further ado let‚Äôs get started. To begin our journey, we need a dataset. Here I‚Äôm taking a small dataset with only 6 instances and 5 features.\n",
      "          \n",
      "          As you can see the target variable y takes 2 values 0 and 1 hence it‚Äôs a binary classification problem. First of all, we need to understand why do we even need the random forest when we already have decision trees. Let‚Äôs draw the decision tree for this dataset. Now if you don‚Äôt know what a decision tree really is or how it is trained then I‚Äôd highly recommend you to watch my previous video. In short, a decision tree splits the dataset recursively using the decision nodes unless we are left with pure leaf nodes.\n",
      "          \n",
      "          And it finds the best split by maximizing the entropy gain. If a data sample satisfies the condition at a decision node then it moves to the left child else it moves to the right and finally reaches a leaf node where a class label is assigned to it. So, what‚Äôs the problem with decision trees? Let‚Äôs change our training data slightly. Focus on the row with id 1. We are changing the x0 and x1 features. Now if we train our tree on this modified dataset we‚Äôll get a completely different tree. This shows us that decision trees are highly sensitive to the training data which could\n",
      "          \n",
      "          result in high variance. So our model might fail to generalize. Here comes the random forest algorithm. It is a collection of multiple random decision trees and it‚Äôs much less sensitive to the training data. You can guess that we use multiple trees hence the name forest. But why it‚Äôs called random? Keep this question in the back of your mind you‚Äôll get the answer by the end of this video. Let me show you the process of creating a random forest. The first step is to build new datasets from our original data.\n",
      "          \n",
      "          To maintain simplicity we‚Äôll build only 4. We are gonna randomly select rows from the original data to build our new datasets. And every dataset will contain the same number of rows as the original one. Here‚Äôs the first dataset. Due to lack of space, I‚Äôm writing only the row ids. Notice that, row 2 and 5 came more than once that‚Äôs because we are performing random sampling with replacement. That means after selecting a row we are putting it back into the data. And here are the rest of the datasets.\n",
      "          \n",
      "          The process we just followed to create new data is called Bootstrapping. Now we‚Äôll train a decision tree on each of the bootstrapped datasets independently. But here‚Äôs a twist we won‚Äôt use every feature for training the trees. We‚Äôll randomly select a subset of features for each tree and use only them for training. For example, in the first case, we‚Äôll only use the features x0, x1. Similarly, here are the subsets used for the remaining trees. Now that we have got the data and the feature subsets let‚Äôs build the trees.\n",
      "          \n",
      "          Just see how different the trees look from each other. And this my friend is the random forest containing 4 trees. But how to make a prediction using this forest? Let‚Äôs take a new data point. We‚Äôll pass this data point through each tree one by one and note down the predictions. Now we have to combine all the predictions. As it‚Äôs a classification problem we‚Äôll take the majority voting. Clearly, 1 is the winner hence the prediction from our random forest is 1. This process of combining results from multiple models is called aggregation.\n"
     ]
    }
   ],
   "source": [
    "path = r\"experiment\\Random Forest\\Random_Forest.txt\"\n",
    "chunks = load_and_split_file(file_path = path )\n",
    "print(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936178ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total character : 5649\n",
      "\n",
      "üîπ Processing chunk 1/2...\n",
      "parser is running\n",
      "raw mcqs :  10\n",
      "parser is running\n",
      "‚úÖ Chunk 1: 10 valid MCQs. Total so far: 10 and raw MCQs are 10.\n",
      "Saved to temp.json\n"
     ]
    }
   ],
   "source": [
    "path = r\"experiment\\Random Forest\\Random_Forest.txt\"\n",
    "mcqs,raw = generate_mcqs_from_file_with_eval(file_path = path,chunk_size=4000)\n",
    "save_to_json(mcqs, \"temp.json\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613b0a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total character : 5649\n",
      "\n",
      "üîπ Processing chunk 1/2...\n",
      "parser is running\n",
      "raw mcqs :  9\n",
      "parser is running\n",
      "‚úÖ Chunk 1: 8 valid MCQs. Total so far: 8 and raw MCQs are 9.\n",
      "\n",
      "üîπ Processing chunk 2/2...\n",
      "parser is running\n",
      "raw mcqs :  8\n",
      "parser is running\n",
      "‚úÖ Chunk 2: 4 valid MCQs. Total so far: 12 and raw MCQs are 17.\n",
      "Saved to Random_Forest_rejected.json\n",
      "Saved to Random_Forest.json\n",
      "Saved to Random_Forest_raw.json\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:\\Users\\admin\\OneDrive\\Desktop\\coriolis\\VS CODE\\experiment\\Random_Forest.txt\"\n",
    "mcqs,raw = generate_mcqs_from_file_with_eval(file_path = path,chunk_size=4000)\n",
    "rejected = get_rejected_mcqs_from_lists(raw,mcqs)\n",
    "save_to_json(rejected, \"Random_Forest_rejected.json\" )\n",
    "save_to_json(mcqs, \"Random_Forest.json\" )\n",
    "save_to_json(raw, \"Random_Forest_raw.json\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ccf9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
